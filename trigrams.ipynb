{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Third-order letter approximation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First cell defines needed parameters such as paths to files.\n",
    "A dictionary to add each text to.\n",
    "And Strings to define preamble and postamble markers for removal.\n",
    "Parameters for word generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file paths to the texts to be used for the trigram\n",
    "files = [\n",
    "    '/workspaces/emergingtechnologies/data/frankenstein.txt',\n",
    "    '/workspaces/emergingtechnologies/data/bookplates.txt',\n",
    "    '/workspaces/emergingtechnologies/data/spyglassmoutain.txt',\n",
    "    '/workspaces/emergingtechnologies/data/theoldhouse.txt',\n",
    "    '/workspaces/emergingtechnologies/data/windofdestiny.txt',\n",
    "]\n",
    "#file path to the list of words to be used for the trigram comparisons\n",
    "Comparisons = '/workspaces/emergingtechnologies/data/words.txt'\n",
    "\n",
    "#dictionary to store the texts\n",
    "texts = {}\n",
    "\n",
    "#preamble and postamble markers\n",
    "start_marker = \" ***\"\n",
    "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "\n",
    "#parameters for the word generator\n",
    "startseed = \"TH\"\n",
    "length = 10000\n",
    "\n",
    "#file path to the json file to store the trigram\n",
    "jsonFile = '/workspaces/emergingtechnologies/trigrams.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to read each file. Rather than doing this individually, I've created a loop which adds each text to a dictionary.I've done it this way so I don't have to deal with individual texts and can simply loop over the dictionary when required to retrieve each text.\n",
    "\n",
    "After this, I used the `.find` feature within the same loop, to locate the markers that I've predefined and extract all text between these markers using `.strip`. This removes the preamble and postamble. I then add this cleaned text to the dictionary with the filename as the key. This allows me to print each text  separately to check each is cleaned and for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read each file and remove preamble and postamble\n",
    "for file_path in files:\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "        #find the start and end markers\n",
    "        start = text.find(start_marker)\n",
    "        end = text.find(end_marker)\n",
    "\n",
    "        #skip the start marker\n",
    "        start += len(start_marker)\n",
    "        #extract all text between the markers\n",
    "        text = text[start:end].strip()\n",
    "\n",
    "        #store the cleaned text in the dictionary with filename as the key\n",
    "        filename = file_path.split('/')[-1]  # Get filename from path\n",
    "        texts[filename] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then create a new loop that iterates over each key-value pair in the `texts` dictionary. I had to add a `.replace` method to this to remove `\\n` as it was showing up a number of times. I then define the characters allowed in text to be fed to the model. After converting my text to uppercase, I clean it by removing any characters not in the defined character set. Finally I add the cleaned text back to its corresponding key in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, text in texts.items():\n",
    "    #remove all newlines\n",
    "    text = text.replace('\\n', ' ')    \n",
    "    text = text.upper()\n",
    "    chars = set('ABCDEFGHIJKLMNOPQRSTUVWXYZ. ')\n",
    "    #remove all characters that are not in the set of allowed characters\n",
    "    cleaned = ''.join(c for c in text if c in chars)\n",
    "    #store the cleaned text back in the dictionary\n",
    "    texts[filename] = cleaned\n",
    "\n",
    "#print(texts['theoldhouse.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then created a trigram model. Using the `model` dictionary and a similar key-value loop as before, I created a loop that iterates over each files text. Each trigram is extracted from the text. A `if` statement was required to remove empty values missed during the cleaning process. These trigrams are then added to the dictionary as a keys, with each occurance being counted and added to the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "\n",
    "for filename, cleaned in texts.items():\n",
    "  #iterate over the text and count the occurrences of each trigram\n",
    "  for i in range(len(cleaned) - 2):\n",
    "    #extract the trigram\n",
    "    trigram = cleaned[i:i+3]\n",
    "    if trigram != '' and trigram != '   ':\n",
    "        #store the trigram in the model\n",
    "      model[trigram] = model.get(trigram, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a sorted example of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(model.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Third-order letter approximation generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first created a trigram dictionary to store the bigrams with counts of next characters, I then looped over the existing trigram and extracted each bigram. Next, I collected the character following each bigram and stored it as `next_char`. Finally, I added the bigram as the key, with the next character as a nested key with its count as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trigram look up dictionary for model\n",
    "trigram_dict = {}\n",
    "\n",
    "#loop through the model and its count in the model\n",
    "for trigram, count in model.items():\n",
    "    #exsure trigram has 3 characters\n",
    "    if len(trigram) == 3:\n",
    "        #extract the bigram\n",
    "        bigram = trigram[:2]\n",
    "        #extract the next character\n",
    "        next_char = trigram[2]\n",
    "        #check bigram is not already accounted for\n",
    "        if bigram not in trigram_dict:\n",
    "            #add the bigram to the dictionary\n",
    "            trigram_dict[bigram] = {}\n",
    "        #add the next character to the dictionary\n",
    "        trigram_dict[bigram][next_char] = count\n",
    "\n",
    "# Manually format and print the trigram dictionary\n",
    "#for bigram, next_chars in trigram_dict.items():\n",
    "    #print(f\"Bigram '{bigram}':\")\n",
    "    #for next_char, count in next_chars.items():\n",
    "        #print(f\"  '{next_char}': {count}\")\n",
    "    #print()  # Blank line for readability between bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then needed to whether I could generate text. I started by adding the `start_seed` which is `TH` to the generated text to ensure the generated text always begins with `TH`.  Next, I used this generated text as the bigram to look up the possible next characters and their weights. I then randomly selected the next character using the weights as probabilities. This next character is appended to the generated text. Printing the results shows a trigram with the last letter randomly generated from the model based on weights. Once I confirmed I can generated the next character. I'll create a loop of this process to repeatly generated a new letter until 10,000 characters are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE\n"
     ]
    }
   ],
   "source": [
    "#initialize the generated text with the start seed\n",
    "generatedtext = startseed\n",
    "\n",
    "#get the last two characters in the generated text\n",
    "bigram = generatedtext[-2:]\n",
    "\n",
    "#get possible next characters and their counts\n",
    "next_chars = list(trigram_dict[bigram].keys())\n",
    "weights = list(trigram_dict[bigram].values())\n",
    "    \n",
    "#randomly select the next character based on weights\n",
    "next_char = random.choices(next_chars, weights=weights, k=1)[0]\n",
    "    \n",
    "#next character to the generated text\n",
    "generatedtext += next_char\n",
    "\n",
    "\n",
    "#print the generated text with one additional character\n",
    "print(generatedtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I essentially do the same as in the last cell, but this time, I add it to a loop that generates to the specified lenght of 10,000 charecters. This means the model will keep reading the last two letters in generated text and generating a third letter based on probabilities from the weights, untill the loop is broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop to generate characters until the length is reached\n",
    "for _ in range(length):\n",
    "    \n",
    "    #get the last two characters in the generated text to form a bigram\n",
    "    bigram = generatedtext[-2:]\n",
    "    #get possible character and weights from trigram_dict\n",
    "    next_chars = list(trigram_dict[bigram].keys()) #list of possible next characters for the bigram\n",
    "    weights = list(trigram_dict[bigram].values())  #counts used as weights\n",
    "    #select the next character based on the weights as probabilities\n",
    "    next_char = random.choices(next_chars, weights=weights, k=1)[0]\n",
    "    \n",
    "    #chosen character to the generated text\n",
    "    generatedtext += next_char\n",
    "\n",
    "#print(generatedtext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Analyze your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I load the words from the text file into a data structure so I can use them for \"comparison with the words generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set to store the comparison words\n",
    "ComparisonSet = set()\n",
    "\n",
    "#read the comparison file\n",
    "with open(Comparisons, 'r') as f:\n",
    "    #loop through the file and add each line to the set\n",
    "    for line in f:\n",
    "        ComparisonSet.add(line.strip())\n",
    "\n",
    "#print(ComparisonSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then use the `.split` method to split the generated text into words. I then initialize a variable to count the valid words. I  loop over the words in `generatedtext` and compare them to the words in the comparison set. If the word is vaild, I increment the vaild word counter. This is how I count the words in the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid words: 654\n"
     ]
    }
   ],
   "source": [
    "#split the generated text into words\n",
    "generatedWords = generatedtext.split()\n",
    "\n",
    "#count for vaild words\n",
    "validWords = 0\n",
    "for word in generatedWords:\n",
    "    #check if the word is in the comparison set\n",
    "    if word in ComparisonSet:\n",
    "        #increment the valid word count\n",
    "        validWords += 1\n",
    "\n",
    "print(f\"Valid words: {validWords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the total amount of valid words, I can use simple average equation: `Average = Sum of Values/Number of Values`. First, I need to get the total words from the generated text, which I can do by getting its length. Then, I simply enter the valid numbers and total numbers variables into the equation to calculate the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 1822\n",
      "Valid words: 654\n",
      "Percentage of valid words: 35.89%\n"
     ]
    }
   ],
   "source": [
    "#get total words in the generated text\n",
    "totalWords = len(generatedWords)\n",
    "#calculate the percentage of valid words\n",
    "percentage = validWords / totalWords * 100\n",
    "\n",
    "print(f\"Total words: {totalWords}\")\n",
    "print(f\"Valid words: {validWords}\")\n",
    "print(f\"Percentage of valid words: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4:  Export your model as JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export my model as JSON, I first open the a `.json` file with the `open` funtion. Once the file open, I can use the `json.dump` method of the `json` module to write the model to the file to the file with proper formatting (pretty print and indentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the trigram dictionary to a json file\n",
    "with open(jsonFile, 'w') as f:\n",
    "    json.dump(trigram_dict, f, indent=4) #pretty print the json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is saved to the `.json` file we need to verify that the model in the file is correct. I do this by loading the model from the file into a dictionary using `json.load`. I can then compare the loaded model with the original model and print r\n",
    "the result to check if they match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#verify the json file is the same as the dictionary\n",
    "with open(jsonFile, 'r') as f:\n",
    "    trigram_dict2 = json.load(f)\n",
    "\n",
    "print(trigram_dict == trigram_dict2) #should print True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has successfully walked through the steps of generating text using a trigram model, validating the generated words, and exporting the model into a JSON file. By following these steps outlined for these tasks I have created a model capable of generating text and evaluating its quality based on the real words provided."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
