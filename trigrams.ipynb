{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Third-order letter approximation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First cell defines needed perameters such as links to files.\n",
    "A dictionary to add each text to.\n",
    "And Strings to define preamble and postable markers for removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    '/workspaces/emergingtechnologies/data/frankenstein.txt',\n",
    "    '/workspaces/emergingtechnologies/data/bookplates.txt',\n",
    "    '/workspaces/emergingtechnologies/data/spyglassmoutain.txt',\n",
    "    '/workspaces/emergingtechnologies/data/theoldhouse.txt',\n",
    "    '/workspaces/emergingtechnologies/data/windofdestiny.txt',\n",
    "]\n",
    "\n",
    "Comparisons = '/workspaces/emergingtechnologies/data/words.txt'\n",
    "\n",
    "texts = {}\n",
    "\n",
    "#preamble and postamble markers\n",
    "start_marker = \" ***\"\n",
    "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "\n",
    "startseed = \"TH\"\n",
    "length = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to read each file, rather then doing this individually I've created a loop which adds each text to a dictionary.I've done it this way so I dont have to deal with indivdual texts and can simply loop over the dictionary when required to retrieve each text.\n",
    "\n",
    "After this I used the .find feature within the same loop, to find the markers that I've predefined and extract all text between these markers using .strip, this removes the preamble and postamble. I then add this cleaned text to the dictionary with filename as the key as this allows me to print each text separatly to check each is cleaned and for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read each file and remove preamble and postamble\n",
    "for file_path in files:\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "        #find the start and end markers\n",
    "        start = text.find(start_marker)\n",
    "        end = text.find(end_marker)\n",
    "\n",
    "        #skip the start marker\n",
    "        start += len(start_marker)\n",
    "        #extract all text between the markers\n",
    "        text = text[start:end].strip()\n",
    "\n",
    "        #store the cleaned text in the dictionary with filename as the key\n",
    "        filename = file_path.split('/')[-1]  # Get filename from path\n",
    "        texts[filename] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then create a new loop that iterates over each key-value pair in the texts dictionary. I had to add a .replace method to this to remove '/n' at it was showing up a number of times. I then define the charecters allowed in text to be fed to the model. Then changing my text to uppercase I clean the the text by removing any charecters not in the defined char set, before adding the cleaned text back to its key in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, text in texts.items():\n",
    "    #remove all newlines\n",
    "    text = text.replace('\\n', ' ')    \n",
    "    text = text.upper()\n",
    "    chars = set('ABCDEFGHIJKLMNOPQRSTUVWXYZ. ')\n",
    "    #remove all characters that are not in the set of allowed characters\n",
    "    cleaned = ''.join(c for c in text if c in chars)\n",
    "    #store the cleaned text back in the dictionary\n",
    "    texts[filename] = cleaned\n",
    "\n",
    "#print(texts['theoldhouse.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then created a trigram model. Using the dictionary model and the same key value loop as before, I created a loop that iterates over the file provided in each loop, each trigram is extracted from the text. A if statement was required to remove empty values that were missed in the cleaning process. These trigrams are then added to the dictionary as a key with each occurance being counted and added to the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "\n",
    "for filename, cleaned in texts.items():\n",
    "  #iterate over the text and count the occurrences of each trigram\n",
    "  for i in range(len(cleaned) - 2):\n",
    "    #extract the trigram\n",
    "    trigram = cleaned[i:i+3]\n",
    "    if trigram != '' and trigram != '   ':\n",
    "        #store the trigram in the model\n",
    "      model[trigram] = model.get(trigram, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a sorted example of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(model.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Third-order letter approximation generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first created a trigram dictionary to store the bigrams with next char counts, I then loop over the existing trigram and extract each bigram from it. I then collect the next charecter from each bigram and store it as next char, I then add the bigram as the key with the next char as the value and its count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trigram look up dictionary for model\n",
    "trigram_dict = {}\n",
    "\n",
    "#loop through the model and its count in the model\n",
    "for trigram, count in model.items():\n",
    "    #exsure trigram has 3 characters\n",
    "    if len(trigram) == 3:\n",
    "        #extract the bigram\n",
    "        bigram = trigram[:2]\n",
    "        #extract the next character\n",
    "        next_char = trigram[2]\n",
    "        #check bigram is not already accounted for\n",
    "        if bigram not in trigram_dict:\n",
    "            #add the bigram to the dictionary\n",
    "            trigram_dict[bigram] = {}\n",
    "        #add the next character to the dictionary\n",
    "        trigram_dict[bigram][next_char] = count\n",
    "\n",
    "# Manually format and print the trigram dictionary\n",
    "#for bigram, next_chars in trigram_dict.items():\n",
    "    #print(f\"Bigram '{bigram}':\")\n",
    "    #for next_char, count in next_chars.items():\n",
    "        #print(f\"  '{next_char}': {count}\")\n",
    "    #print()  # Blank line for readability between bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then need to test to see if I can generate text, I add the startseed which is TH to the generated text to exsure it always starts with TH. I then add this generated text to the bigram to look up the next chars and weights, I then randomly select a next charecter using the weights as probabilities I then add this next charecter to the generated text print this shows a trigram with the last letter randomly selected. Now that I have shown I can generate a next charecter I'll loop this process to repeat until 10,000 charecters are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE\n"
     ]
    }
   ],
   "source": [
    "#initialize the generated text with the start seed\n",
    "generatedtext = startseed\n",
    "\n",
    "#get the last two characters in the generated text\n",
    "bigram = generatedtext[-2:]\n",
    "\n",
    "#get possible next characters and their counts\n",
    "next_chars = list(trigram_dict[bigram].keys())\n",
    "weights = list(trigram_dict[bigram].values())\n",
    "    \n",
    "#randomly select the next character based on weights\n",
    "next_char = random.choices(next_chars, weights=weights, k=1)[0]\n",
    "    \n",
    "#next character to the generated text\n",
    "generatedtext += next_char\n",
    "\n",
    "\n",
    "#print the generated text with one additional character\n",
    "print(generatedtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a essentially do the exact same as in the last cell but add it to a loop with the specified lenght of 10,000 charecters, this means the model will keep reading the last 2 letters in generated text and generating a third letter based on probability from the weights untill the loop is broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop to generate characters until the length is reached\n",
    "for _ in range(length):\n",
    "    \n",
    "    #get the last two characters in the generated text to form a bigram\n",
    "    bigram = generatedtext[-2:]\n",
    "    #get possible character and weights from trigram_dict\n",
    "    next_chars = list(trigram_dict[bigram].keys()) #list of possible next characters for the bigram\n",
    "    weights = list(trigram_dict[bigram].values())  #counts used as weights\n",
    "    #select the next character based on the weights as probabilities\n",
    "    next_char = random.choices(next_chars, weights=weights, k=1)[0]\n",
    "    \n",
    "    #chosen character to the generated text\n",
    "    generatedtext += next_char\n",
    "\n",
    "#print(generatedtext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Analyze your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set to store the comparison words\n",
    "ComparisonSet = set()\n",
    "\n",
    "#read the comparison file\n",
    "with open(Comparisons, 'r') as f:\n",
    "    #loop through the file and add each line to the set\n",
    "    for line in f:\n",
    "        ComparisonSet.add(line.strip())\n",
    "\n",
    "#print(ComparisonSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid words: 696\n"
     ]
    }
   ],
   "source": [
    "#split the generated text into words\n",
    "generatedWords = generatedtext.split()\n",
    "\n",
    "#count for vaild words\n",
    "validWords = 0\n",
    "for word in generatedWords:\n",
    "    #check if the word is in the comparison set\n",
    "    if word in ComparisonSet:\n",
    "        #increment the valid word count\n",
    "        validWords += 1\n",
    "\n",
    "print(f\"Valid words: {validWords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 1851\n",
      "Percentage of valid words: 37.60%\n"
     ]
    }
   ],
   "source": [
    "#get total words in the generated text\n",
    "totalWords = len(generatedWords)\n",
    "#calculate the percentage of valid words\n",
    "percentage = validWords / totalWords * 100\n",
    "\n",
    "print(f\"Total words: {totalWords}\")\n",
    "print(f\"Percentage of valid words: {percentage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
